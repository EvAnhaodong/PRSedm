{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097f5754-667b-4fe0-af76-2e85483b94dd",
   "metadata": {},
   "source": [
    "# PRS for imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddcc40-63b5-47d5-a2bc-83b7f6a52a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Hail requirements on instance\n",
    "import os\n",
    "os.system('wget https://github.com/adoptium/temurin21-binaries/releases/download/jdk-21.0.4%2B7/OpenJDK21U-jre_x64_linux_hotspot_21.0.4_7.tar.gz')\n",
    "!tar -xvzf OpenJDK21U-jre_x64_linux_hotspot_21.0.4_7.tar.gz\n",
    "!pip install pyspark\n",
    "# Set the JAVA_HOME environment variable\n",
    "os.environ['JAVA_HOME'] = '/opt/notebooks/jdk-21.0.4+7-jre' \n",
    "os.environ['PATH'] = f\"{os.environ['JAVA_HOME']}/bin:\" + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798da8f9-1a76-466e-93f6-810705fd0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Hail on instance\n",
    "!pip install hail --force-reinstall\n",
    "import hail as hl\n",
    "hl.init(idempotent=True)\n",
    "hl.default_reference('GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4ed40-5206-49fa-8813-81d22002154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download .bgen and .sample files for imputed data for each chromosome 1-22 into instance\n",
    "import os\n",
    "os.system('dx download <bgen file name>')         \n",
    "os.system('dx download <sample file name>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0271a3-93f4-4725-b02c-771b7d564bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through chromosomes 1-22 and index files using Hail\n",
    "for chromosome in range(1, 23):\n",
    "    bgen_file = f'<bgen_name_chr{chromosome}>.bgen'\n",
    "    try:\n",
    "        print(f\"Indexing {bgen_file}...\")\n",
    "        hl.index_bgen(bgen_file)\n",
    "        print(f\"{bgen_file} indexed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error indexing file for chromosome {chromosome}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8826c-e260-4a72-9fcc-9cca63f39bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download PRSedm into instance\n",
    "import os\n",
    "os.system('wget https://files.pythonhosted.org/packages/6e/d9/ebd00d933502674a1072f226bc429e5092ab365941262def01f4cffdbb44/prsedm-1.0.0-py3-none-any.whl')\n",
    "!pip install prsedm\n",
    "import prsedm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ecd6b-1ab3-4a66-a543-9de5cdd51b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "entry_fields = ['GT', 'GP'] \n",
    "mt_subsets = []\n",
    "\n",
    "#Loop through chromosomes 1-22\n",
    "for chromosome in range(1, 23):\n",
    "    bgen_file = f'<bgen_name_chr{chromosome}>.bgen'\n",
    "    sample_file = f'<sample_name_chr{chromosome}>.sample'\n",
    "    #Import BGEN file into Hail\n",
    "    print(f\"Importing {bgen_file} into Hail...\")\n",
    "    try:\n",
    "        mt = hl.import_bgen(bgen_file, sample_file=sample_file, entry_fields=entry_fields)\n",
    "        print(f\"{bgen_file} imported successfully.\")\n",
    "        mt_subsets.append(mt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing {bgen_file}: {e}\")\n",
    "        \n",
    "#Get list of SNPs to extract from PRS model\n",
    "df = prsedm.get_snp_db('t1dgrs2-sharp24')\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "#Build a list of variant positions to extract\n",
    "pos_col='position_hg38' #change to hg19 if required\n",
    "variantIntervals = [\n",
    "    f\"chr{row['contig_id']}:{row[pos_col]}-{row[pos_col] + 1}\"\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "print(f\"Number of unique variants to extract: {len(variantIntervals)}\")\n",
    "\n",
    "#Chunk up the regions incase there are too many variants requested\n",
    "print(\"Retrieve chunks from VDS and densify...\")\n",
    "chunk_size=1000\n",
    "chunked_intervals = [variantIntervals[i:i + chunk_size] for i in range(0, len(variantIntervals), chunk_size)]\n",
    "mt_subsets = []\n",
    "for i,chunk in enumerate(chunked_intervals):\n",
    "    print(f\"Processing chunk: {i+1}\")\n",
    "    bgen_filtered = hl.filter_intervals(mt,[hl.parse_locus_interval(x) for x in chunk])\n",
    "    bgen_filtered=hl.split_multi_hts(bgen_filtered)\n",
    "    mt_subsets.append(bgen_filtered)\n",
    "\n",
    "\n",
    "#Combine chunks\n",
    "print(\"Combining retrieved chunks...\")\n",
    "if mt_subsets:\n",
    "    combined_mt = mt_subsets[0]\n",
    "    for mt_n in mt_subsets[1:]:\n",
    "        combined_mt = combined_mt.union_rows(mt_n)\n",
    "\n",
    "    #Process the merged data\n",
    "    print(\"Processing merged MT...\")\n",
    "    combined_mt = hl.variant_qc(combined_mt)\n",
    "    combined_mt = combined_mt.annotate_rows(info=hl.struct(AF=combined_mt.variant_qc.AF))\n",
    "\n",
    "    #Export to VCF\n",
    "    print(\"Exporting to VCF...\")\n",
    "    start = datetime.now()\n",
    "    hl.export_vcf(combined_mt, f'./chr{chromosome}_temp.vcf.bgz')\n",
    "    print(f\"Export took {(datetime.now() - start).total_seconds():.2f} seconds\")\n",
    "else:\n",
    "    print(\"No MatrixTables were imported; nothing to combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c197d4-b085-49fb-8010-62d220ffb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use bcftools to merge VCF files\n",
    "!bcftools concat -Oz chr1_temp.vcf.bgz chr2_temp.vcf.bgz chr3_temp.vcf.bgz chr4_temp.vcf.bgz chr5_temp.vcf.bgz chr6_temp.vcf.bgz chr7_temp.vcf.bgz chr8_temp.vcf.bgz chr9_temp.vcf.bgz chr10_temp.vcf.bgz chr11_temp.vcf.bgz chr12_temp.vcf.bgz chr13_temp.vcf.bgz chr14_temp.vcf.bgz chr15_temp.vcf.bgz chr16_temp.vcf.bgz chr17_temp.vcf.bgz chr18_temp.vcf.bgz chr19_temp.vcf.bgz chr20_temp.vcf.bgz chr21_temp.vcf.bg chr22_temp.vcf.bgz -o all_data.vcf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc541e7c-e865-41a3-834d-2bf1c35e5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index files\n",
    "!tabix -fp vcf all_data.vcf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b79e1-2959-44d4-9088-bc460a75aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download reference TOPMED data using command for individual account and then \n",
    "#Index files\n",
    "!for f in reference.vcf.gz;do tabix -f $f;done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea8a1a-8252-40f2-ab34-7078e4a5db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate GRS\n",
    "output = prsedm.gen_dm(vcf=vcf, \n",
    "                            col=\"GP\", \n",
    "                            build=\"hg38\", \n",
    "                            prsflags=\"t1dgrs2-luckett25\", \n",
    "                            impute=1, \n",
    "                            refvcf=<path to TOPMED reference files>,\n",
    "                            norm=1,\n",
    "                            ntasks=16,\n",
    "                            parallel=1,\n",
    "                            batch_size=1)\n",
    "#Save results\n",
    "output.to_csv(f\"prsedm_result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
